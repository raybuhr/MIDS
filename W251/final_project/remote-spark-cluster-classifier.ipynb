{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS 251 Final Project\n",
    "## Analyzing Reddit Data\n",
    "### Creating a prediction model for subreddit using pyspark.ml.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"/python\")\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python/lib/py4j-0.8.2.1-src.zip\"))\n",
    "os.chdir(spark_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.5 (default, Jun 24 2015 00:41:19)\n",
      "SparkContext available as sc, SQLContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "execfile(os.path.join(spark_home, \"python/pyspark/shell.py\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/spark-1.5.3-SNAPSHOT-bin-spark-1.5.2-hadoop-2.6.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "text_file = sc.textFile(\"file:///\" + spark_home + \"/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_counts = text_file \\\n",
    "   .flatMap(lambda line: line.split()) \\\n",
    "   .map(lambda word: (word, 1)) \\\n",
    "   .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'when', 1)\n",
      "(u'R,', 1)\n",
      "(u'including', 3)\n",
      "(u'computation', 1)\n",
      "(u'using:', 1)\n",
      "(u'guidance', 3)\n",
      "(u'Scala,', 1)\n",
      "(u'environment', 1)\n",
      "(u'only', 1)\n",
      "(u'rich', 1)\n",
      "(u'Apache', 1)\n",
      "(u'sc.parallelize(range(1000)).count()', 1)\n",
      "(u'Building', 1)\n",
      "(u'guide,', 1)\n",
      "(u'return', 2)\n",
      "(u'Please', 3)\n",
      "(u'Try', 1)\n",
      "(u'not', 1)\n",
      "(u'Spark', 14)\n",
      "(u'scala>', 1)\n",
      "(u'Note', 1)\n",
      "(u'cluster.', 1)\n",
      "(u'./bin/pyspark', 1)\n",
      "(u'have', 1)\n",
      "(u'params', 1)\n",
      "(u'through', 1)\n",
      "(u'GraphX', 1)\n",
      "(u'[run', 1)\n",
      "(u'abbreviated', 1)\n",
      "(u'[project', 2)\n",
      "(u'##', 8)\n",
      "(u'library', 1)\n",
      "(u'see', 1)\n",
      "(u'\"local\"', 1)\n",
      "(u'[Apache', 1)\n",
      "(u'will', 1)\n",
      "(u'#', 1)\n",
      "(u'processing,', 1)\n",
      "(u'for', 12)\n",
      "(u'[building', 1)\n",
      "(u'provides', 1)\n",
      "(u'print', 1)\n",
      "(u'supports', 2)\n",
      "(u'built,', 1)\n",
      "(u'[params]`.', 1)\n",
      "(u'available', 1)\n",
      "(u'run', 7)\n",
      "(u'tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).', 1)\n",
      "(u'This', 2)\n",
      "(u'Hadoop,', 2)\n",
      "(u'Tests', 1)\n",
      "(u'example:', 1)\n",
      "(u'-DskipTests', 1)\n",
      "(u'Maven](http://maven.apache.org/).', 1)\n",
      "(u'programming', 1)\n",
      "(u'running', 1)\n",
      "(u'against', 1)\n",
      "(u'site,', 1)\n",
      "(u'comes', 1)\n",
      "(u'package.', 1)\n",
      "(u'and', 10)\n",
      "(u'package.)', 1)\n",
      "(u'prefer', 1)\n",
      "(u'documentation,', 1)\n",
      "(u'submit', 1)\n",
      "(u'tools', 1)\n",
      "(u'use', 3)\n",
      "(u'from', 1)\n",
      "(u'For', 2)\n",
      "(u'fast', 1)\n",
      "(u'systems.', 1)\n",
      "(u'<http://spark.apache.org/>', 1)\n",
      "(u'Hadoop-supported', 1)\n",
      "(u'way', 1)\n",
      "(u'README', 1)\n",
      "(u'MASTER', 1)\n",
      "(u'engine', 1)\n",
      "(u'building', 3)\n",
      "(u'usage', 1)\n",
      "(u'Distributions\"](http://spark.apache.org/docs/latest/hadoop-third-party-distributions.html)', 1)\n",
      "(u'instance:', 1)\n",
      "(u'with', 4)\n",
      "(u'protocols', 1)\n",
      "(u'And', 1)\n",
      "(u'this', 1)\n",
      "(u'setup', 1)\n",
      "(u'shell:', 2)\n",
      "(u'project', 1)\n",
      "(u'See', 1)\n",
      "(u'following', 2)\n",
      "(u'distribution', 1)\n",
      "(u'detailed', 2)\n",
      "(u'file', 1)\n",
      "(u'stream', 1)\n",
      "(u'is', 6)\n",
      "(u'higher-level', 1)\n",
      "(u'tests', 2)\n",
      "(u'1000:', 2)\n",
      "(u'sample', 1)\n",
      "(u'[\"Specifying', 1)\n",
      "(u'Alternatively,', 1)\n",
      "(u'./bin/run-example', 2)\n",
      "(u'need', 1)\n",
      "(u'You', 3)\n",
      "(u'instructions.', 1)\n",
      "(u'different', 1)\n",
      "(u'programs,', 1)\n",
      "(u'storage', 1)\n",
      "(u'same', 1)\n",
      "(u'machine', 1)\n",
      "(u'Running', 1)\n",
      "(u'which', 2)\n",
      "(u'you', 4)\n",
      "(u'A', 1)\n",
      "(u'About', 1)\n",
      "(u'sc.parallelize(1', 1)\n",
      "(u'locally.', 1)\n",
      "(u'Hive', 2)\n",
      "(u'optimized', 1)\n",
      "(u'uses', 1)\n",
      "(u'Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)', 1)\n",
      "(u'variable', 1)\n",
      "(u'The', 1)\n",
      "(u'data', 1)\n",
      "(u'a', 10)\n",
      "(u'Thriftserver', 1)\n",
      "(u'processing.', 1)\n",
      "(u'./bin/spark-shell', 1)\n",
      "(u'Python', 2)\n",
      "(u'Spark](#building-spark).', 1)\n",
      "(u'clean', 1)\n",
      "(u'the', 21)\n",
      "(u'requires', 1)\n",
      "(u'talk', 1)\n",
      "(u'help', 1)\n",
      "(u'Hadoop', 4)\n",
      "(u'using', 2)\n",
      "(u'high-level', 1)\n",
      "(u'find', 1)\n",
      "(u'web', 1)\n",
      "(u'Shell', 2)\n",
      "(u'how', 2)\n",
      "(u'graph', 1)\n",
      "(u'run:', 1)\n",
      "(u'should', 2)\n",
      "(u'to', 14)\n",
      "(u'module,', 1)\n",
      "(u'given.', 1)\n",
      "(u'directory.', 1)\n",
      "(u'must', 1)\n",
      "(u'do', 2)\n",
      "(u'Programs', 1)\n",
      "(u'Many', 1)\n",
      "(u'\"yarn-client\"', 1)\n",
      "(u'YARN,', 1)\n",
      "(u'[\"Third', 1)\n",
      "(u'Example', 1)\n",
      "(u'Once', 1)\n",
      "(u'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1)\n",
      "(u'Because', 1)\n",
      "(u'name', 1)\n",
      "(u'Testing', 1)\n",
      "(u'refer', 2)\n",
      "(u'Streaming', 1)\n",
      "(u'SQL', 2)\n",
      "(u'them,', 1)\n",
      "(u'analysis.', 1)\n",
      "(u'application', 1)\n",
      "(u'set', 2)\n",
      "(u'Scala', 2)\n",
      "(u'thread,', 1)\n",
      "(u'individual', 1)\n",
      "(u'examples', 2)\n",
      "(u'changed', 1)\n",
      "(u'runs.', 1)\n",
      "(u'Pi', 1)\n",
      "(u'More', 1)\n",
      "(u'Python,', 2)\n",
      "(u'Versions', 1)\n",
      "(u'its', 1)\n",
      "(u'version', 1)\n",
      "(u'wiki](https://cwiki.apache.org/confluence/display/SPARK).', 1)\n",
      "(u'`./bin/run-example', 1)\n",
      "(u'Configuration', 1)\n",
      "(u'command,', 2)\n",
      "(u'can', 6)\n",
      "(u'core', 1)\n",
      "(u'Guide](http://spark.apache.org/docs/latest/configuration.html)', 1)\n",
      "(u'MASTER=spark://host:7077', 1)\n",
      "(u'Documentation', 1)\n",
      "(u'downloaded', 1)\n",
      "(u'distributions.', 1)\n",
      "(u'Spark.', 1)\n",
      "(u'[\"Building', 1)\n",
      "(u'`examples`', 2)\n",
      "(u'on', 6)\n",
      "(u'works', 1)\n",
      "(u'package', 1)\n",
      "(u'of', 5)\n",
      "(u'APIs', 1)\n",
      "(u'pre-built', 1)\n",
      "(u'Big', 1)\n",
      "(u'\"yarn-cluster\"', 1)\n",
      "(u'or', 4)\n",
      "(u'learning,', 1)\n",
      "(u'locally', 2)\n",
      "(u'overview', 1)\n",
      "(u'one', 2)\n",
      "(u'(You', 1)\n",
      "(u'Online', 1)\n",
      "(u'versions', 1)\n",
      "(u'your', 1)\n",
      "(u'threads.', 1)\n",
      "(u'>>>', 1)\n",
      "(u'SparkPi', 2)\n",
      "(u'contains', 1)\n",
      "(u'system', 1)\n",
      "(u'class', 2)\n",
      "(u'start', 1)\n",
      "(u'build/mvn', 1)\n",
      "(u'basic', 1)\n",
      "(u'configure', 1)\n",
      "(u'that', 3)\n",
      "(u'N', 1)\n",
      "(u'./dev/run-tests', 1)\n",
      "(u'DataFrames,', 1)\n",
      "(u'particular', 3)\n",
      "(u'be', 2)\n",
      "(u'an', 3)\n",
      "(u'easiest', 1)\n",
      "(u'Interactive', 2)\n",
      "(u'cluster', 2)\n",
      "(u'page](http://spark.apache.org/documentation.html)', 1)\n",
      "(u'<class>', 1)\n",
      "(u'example', 3)\n",
      "(u'are', 1)\n",
      "(u'Data.', 1)\n",
      "(u'mesos://', 1)\n",
      "(u'computing', 1)\n",
      "(u'URL,', 1)\n",
      "(u'in', 5)\n",
      "(u'general', 2)\n",
      "(u'To', 2)\n",
      "(u'at', 2)\n",
      "(u'1000).count()', 1)\n",
      "(u'Party', 1)\n",
      "(u'if', 4)\n",
      "(u'built', 1)\n",
      "(u'no', 1)\n",
      "(u'Java,', 1)\n",
      "(u'MLlib', 1)\n",
      "(u'also', 5)\n",
      "(u'other', 1)\n",
      "(u'build', 3)\n",
      "(u'online', 1)\n",
      "(u'several', 1)\n",
      "(u'distribution.', 1)\n",
      "(u'HDFS', 1)\n",
      "(u'[Configuration', 1)\n",
      "(u'spark://', 1)\n",
      "(u'programs', 2)\n",
      "(u'documentation', 3)\n",
      "(u'It', 2)\n",
      "(u'graphs', 1)\n",
      "(u'\"local[N]\"', 1)\n",
      "(u'first', 1)\n",
      "(u'latest', 1)\n"
     ]
    }
   ],
   "source": [
    "for wc in word_counts.collect():\n",
    "    print(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       1.0|         1.0|(692,[129,130,131...|\n",
      "|       0.0|         0.0|(692,[158,159,160...|\n",
      "|       1.0|         1.0|(692,[154,155,156...|\n",
      "|       0.0|         0.0|(692,[129,130,131...|\n",
      "|       0.0|         0.0|(692,[97,98,99,12...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0\n",
      "DecisionTreeClassificationModel of depth 2 with 5 nodes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = MLUtils.loadLibSVMFile(sc, \"file:///usr/local/spark/data/mllib/sample_libsvm_data.txt\").toDF()\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"precision\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print \"Test Error = %g\" % (1.0 - accuracy)\n",
    "\n",
    "treeModel = model.stages[2]\n",
    "print treeModel # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "W251_HOST",
   "language": "python",
   "name": "template"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
